{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vjy5Ue6jcJzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/datasets/basel_preprocess.zip /content/"
      ],
      "metadata": {
        "id": "yDqj8M3NeNMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install mysql-connector\n",
        "!pip install Pillow --upgrade"
      ],
      "metadata": {
        "id": "DeG2OsOXYnO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the TensorFlow Model Garden repository\n",
        "!git clone https://github.com/tensorflow/models.git\n"
      ],
      "metadata": {
        "id": "FxZeGmvpZR5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install protobuf-compiler python-lxml python-pil\n",
        "!pip install Cython pandas tf-slim lvis"
      ],
      "metadata": {
        "id": "25rnhr_oZeZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cd into 'TensorFlow/models/research'\n",
        "%cd '/content/models/research'\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "metadata": {
        "id": "4To7nCb3ZiPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ['PYTHONPATH']+=\":/content/models\"\n",
        "sys.path.append(\"/content/models/research\")"
      ],
      "metadata": {
        "id": "2k2niP-xZqz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "0a5uhl6IZzka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/models/research/object_detection/packages/tf2/setup.py build\n",
        "!python /content/models/research/object_detection/packages/tf2/setup.py install"
      ],
      "metadata": {
        "id": "YEPX3FAaZ6Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cd into 'TensorFlow/models/research/object_detection/builders/'\n",
        "%cd '/content/models/research/object_detection/builders/'\n",
        "!python model_builder_tf2_test.py\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "pfwXtE2Ga1Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "id": "a6_A6Q0ZcQD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/abundis-rmn2/graffiti_detection_OD_TF/main/labelmap.pbtxt\"\n",
        "!wget \"https://data.abundis.com.mx/inference_graph.tar.gz\"\n",
        "!wget \"https://data.abundis.com.mx/config.json\"\n",
        "!tar xvzf inference_graph.tar.gz\n",
        "!ls"
      ],
      "metadata": {
        "id": "ykI-CTeXbxKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5HlJ55vX4AO"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import argparse\n",
        "#import mysql.connector\n",
        "import json\n",
        "\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import json\n",
        "\n",
        "from six import BytesIO\n",
        "from PIL import Image\n",
        "Image.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "import ftplib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "  Args:\n",
        "    path: a file path (this can be local or on colossus)\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key: value[0, :num_detections].numpy()\n",
        "                 for key, value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "\n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "      output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "      image.shape[0], image.shape[1])\n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "\n",
        "  return output_dict\n",
        "\n",
        "def directory_exists(dir,ftp):\n",
        "    filelist = []\n",
        "    ftp.retrlines('LIST',filelist.append)\n",
        "    for f in filelist:\n",
        "        if f.split()[-1] == dir and f.upper().startswith('D'):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "'''def DataUpload(local_dir, target_dir):\n",
        "    ftp_server.cwd('/media/exported_images')\n",
        "    if directory_exists(target_dir, ftp_server) is False:  # (or negate, whatever you prefer for readability)\n",
        "        print(target_dir)\n",
        "        ftp_server.mkd(target_dir)\n",
        "    ftp_server.cwd(target_dir)\n",
        "    # https://stackoverflow.com/questions/67520579/uploading-a-files-in-a-folder-to-ftp-using-python-ftplib\n",
        "    print(\"Uploading exported batch\")\n",
        "    toFTP = os.listdir(local_dir)\n",
        "    for filename in toFTP:\n",
        "        if filename not in ftp_server.nlst():\n",
        "            print(\"Uploading: \")\n",
        "            with open(os.path.join(local_dir, filename), 'rb') as file:  # Here I open the file using it's  full path\n",
        "                ftp_server.storbinary(f'STOR {filename}', file)  # Here I store the file in the FTP using only it's name as I intended\n",
        "            print(filename)\n",
        "        else:\n",
        "            print(\"File already exist\")\n",
        "    ftp_server.quit()'''"
      ],
      "metadata": {
        "id": "FNF_5vcobO8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelmap_path = 'labelmap.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)\n",
        "print(\"Load labelmap\")\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.saved_model.load('inference_graph/saved_model')\n",
        "print(\"Load inference graph\")\n",
        "\n",
        "global_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "#c = open(\"config.json\")\n",
        "#config = json.load(c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0kpqotObZNS",
        "outputId": "c6f6b46e-debe-4286-b3d1-287cb1e011c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load labelmap\n",
            "Load inference graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"./downloaded_images/basel_test\", 0o777)\n",
        "!mv /content/basel_preprocess.zip /content/downloaded_images/\n",
        "#!rm -r /content/downloaded_images/basel_preprocess\n",
        "!unzip \"/content/downloaded_images/basel_preprocess.zip\" -d \"/content/downloaded_images/\""
      ],
      "metadata": {
        "id": "CuPOQrGbemGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MUID = \"basel_preprocess\"\n",
        "json_name = 'normalized_dataset_info.json'\n",
        "output_name = 'output_json.json'\n",
        "\n",
        "# Load Dataset Info\n",
        "if (os.path.exists(f'./downloaded_images/{MUID}/{json_name}')):\n",
        "    with open(f'./downloaded_images/{MUID}/{json_name}', \"r\") as file:\n",
        "        pano_json = json.load(file)\n",
        "else:\n",
        "    print(\"JSON for the dataset is missing!\")\n",
        "\n",
        "# Load Output Info\n",
        "if (os.path.exists(f'./downloaded_images/{MUID}/{output_name}')):\n",
        "    with open(f'./downloaded_images/{MUID}/{output_name}', \"r\") as file:\n",
        "        output_json = json.load(file)\n",
        "        #last_item = output_json[-1]\n",
        "        #print(last_item)\n",
        "        #last_id = last_item['id']\n",
        "        pano_json_new = output_json\n",
        "else:\n",
        "    print(\"There is no JSON with Output History!\")\n",
        "    #last_id = 0\n",
        "    pano_json_new = []\n",
        "\n",
        "dir_exist = os.path.exists(\"./exported_images/\" + MUID)\n",
        "if not dir_exist:\n",
        "    #os.makedirs(user_dir, 0o777)\n",
        "    os.makedirs(\"./exported_images/\" + MUID, 0o777)\n",
        "    print(\"The dir was created\")\n",
        "else:\n",
        "    print(\"The dir already exist\")\n",
        "\n",
        "dir_exist = os.path.exists(\"./downloaded_images/\")\n",
        "if not dir_exist:\n",
        "    #os.makedirs(user_dir, 0o777)\n",
        "    os.makedirs(\"./downloaded_images/\", 0o777)\n",
        "    print(\"The dir was created\")\n",
        "else:\n",
        "    print(\"The dir already exist\")\n",
        "\n",
        "\n",
        "    print(\"Looking for images in MUID:\", MUID)\n",
        "    images = glob.glob(f'./downloaded_images/{MUID}/*.jpg')\n",
        "    print(\"MUID found: \", len(images))\n",
        "\n",
        "\n",
        "# Get the current time\n",
        "start_time = time.time()\n",
        "# Set the interval in seconds\n",
        "interval = 120\n",
        "\n",
        "for img in images:\n",
        "  if(images.index(img) > 5960):\n",
        "    os.system('clx')\n",
        "    print(f'{images.index(img)}/{len(images)}')\n",
        "    try:\n",
        "        image_np = load_image_into_numpy_array(img)\n",
        "        output_dict = run_inference_for_single_image(model, image_np)\n",
        "        '''vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "            image_np,\n",
        "            output_dict['detection_boxes'],\n",
        "            output_dict['detection_classes'],\n",
        "            output_dict['detection_scores'],\n",
        "            category_index,\n",
        "            instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "            use_normalized_coordinates=True,\n",
        "            line_thickness=8)\n",
        "        im = Image.fromarray(image_np)\n",
        "        im.save(f'./exported_images/{MUID}/{id(img)}.jpg')'''\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    pano = \"\"\n",
        "    c = 1\n",
        "    for d_class, d_score, d_boxes in zip(output_dict['detection_classes'][:5], output_dict['detection_scores'][:5],output_dict['detection_boxes'][:5]):\n",
        "      if d_score > 0.5:\n",
        "        d_class_name = category_index[d_class]['name']\n",
        "        #print('{0} with score {1}'.format(d_class_name, d_score))\n",
        "\n",
        "        #Structure Output for JSON\n",
        "        obj = {d_class_name: {\n",
        "              'score': float(d_score),\n",
        "              'side': img[-5:-4], #f'./downloaded_images/{MUID}/*.jpg'\n",
        "              'pos': d_boxes.tolist()\n",
        "              }}\n",
        "\n",
        "        #Load JSON Data (if img ids matching > keep)\n",
        "\n",
        "        #PREVENT DUPLICATES!!!\n",
        "        \n",
        "        for pano in pano_json:\n",
        "          if pano['id'] == img.replace(f'./downloaded_images/{MUID}/','')[:-6]:\n",
        "            if pano not in pano_json_new:\n",
        "            # Add the list named \"objects\"\n",
        "              pano['objects'].append(obj)\n",
        "            else:\n",
        "              pano_json_new[pano]['objects'].append(obj)\n",
        "            break\n",
        "    if not pano == \"\":\n",
        "      pano_json_new.append(pano)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Elapsed time: {elapsed_time}')\n",
        "    if elapsed_time >= interval:\n",
        "      # Execute your code here\n",
        "      inference_json = json.dumps(pano_json_new, indent=4)\n",
        "      with open(f'/content/drive/MyDrive/datasets/{output_name}', 'w') as outfile:\n",
        "        current_time = time.time()\n",
        "        outfile.write(inference_json)\n",
        "      print(\"----------- Saving... -----------\")\n",
        "      # Reset the start time\n",
        "      start_time = time.time()\n",
        "    # Wait for a short time to avoid consuming too much CPU time\n",
        "    time.sleep(0.1)\n",
        "      \n",
        "inference_json = json.dumps(pano_json_new, indent=4)\n",
        "with open(f'/content/drive/MyDrive/datasets/{output_name}', 'w') as outfile:\n",
        "    outfile.write(inference_json)"
      ],
      "metadata": {
        "id": "SFu8UGA7bbbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64ec08a4-36e4-4a3f-dfe3-74f14e1da725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dir already exist\n",
            "The dir already exist\n",
            "Looking for images in MUID: basel_preprocess\n",
            "MUID found:  10272\n",
            "5961/10272\n",
            "Elapsed time: 1.3283898830413818\n",
            "5962/10272\n",
            "Elapsed time: 2.007566213607788\n",
            "5963/10272\n",
            "Elapsed time: 2.6909778118133545\n",
            "5964/10272\n",
            "Elapsed time: 3.367452621459961\n",
            "5965/10272\n",
            "Elapsed time: 4.026472091674805\n",
            "5966/10272\n",
            "Elapsed time: 4.607198476791382\n",
            "5967/10272\n",
            "Elapsed time: 5.568461656570435\n",
            "5968/10272\n",
            "Elapsed time: 6.487635374069214\n",
            "5969/10272\n",
            "Elapsed time: 7.162556409835815\n",
            "5970/10272\n",
            "Elapsed time: 7.834930419921875\n",
            "5971/10272\n",
            "Elapsed time: 8.507859230041504\n",
            "5972/10272\n",
            "Elapsed time: 9.18203067779541\n",
            "5973/10272\n",
            "Elapsed time: 10.11753511428833\n",
            "5974/10272\n",
            "Elapsed time: 11.039308071136475\n",
            "5975/10272\n",
            "Elapsed time: 12.308043241500854\n",
            "5976/10272\n",
            "Elapsed time: 13.55767011642456\n",
            "5977/10272\n",
            "Elapsed time: 14.793631076812744\n",
            "5978/10272\n",
            "Elapsed time: 15.960301876068115\n",
            "5979/10272\n",
            "Elapsed time: 17.133535623550415\n",
            "5980/10272\n",
            "Elapsed time: 18.84387469291687\n",
            "5981/10272\n",
            "Elapsed time: 20.010055780410767\n",
            "5982/10272\n",
            "Elapsed time: 20.942370653152466\n",
            "5983/10272\n",
            "Elapsed time: 21.884203672409058\n",
            "5984/10272\n",
            "Elapsed time: 22.554584741592407\n",
            "5985/10272\n",
            "Elapsed time: 23.222498416900635\n",
            "5986/10272\n",
            "Elapsed time: 23.88753652572632\n",
            "5987/10272\n",
            "Elapsed time: 24.59485697746277\n",
            "5988/10272\n",
            "Elapsed time: 25.278003215789795\n",
            "5989/10272\n",
            "Elapsed time: 25.947608709335327\n",
            "5990/10272\n",
            "Elapsed time: 26.635255575180054\n",
            "5991/10272\n",
            "Elapsed time: 27.300097703933716\n",
            "5992/10272\n",
            "Elapsed time: 28.27894139289856\n",
            "5993/10272\n",
            "Elapsed time: 29.246958255767822\n",
            "5994/10272\n",
            "Elapsed time: 30.401355028152466\n",
            "5995/10272\n",
            "Elapsed time: 31.748607397079468\n",
            "5996/10272\n",
            "Elapsed time: 33.02442741394043\n",
            "5997/10272\n",
            "Elapsed time: 34.718647956848145\n",
            "5998/10272\n",
            "Elapsed time: 35.91093158721924\n",
            "5999/10272\n",
            "Elapsed time: 36.629605770111084\n",
            "6000/10272\n",
            "Elapsed time: 36.97456407546997\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-31643f09a4c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Wait for a short time to avoid consuming too much CPU time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0minference_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpano_json_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}